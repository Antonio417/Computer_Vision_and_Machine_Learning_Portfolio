{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75ae5f81",
   "metadata": {},
   "source": [
    "# Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a6e6fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "import keras\n",
    "from keras import backend as k\n",
    "k.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc2d56c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Parameter\n",
    "image_shape = (28,28,1)\n",
    "batch_size = 64\n",
    "latent_dim = 10\n",
    "epoch = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25dec047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MNIST dataset\n",
    "(x_train, y_train), (x_test,y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55141fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change datatype and reshape data\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_train = x_train.reshape((x_train.shape[0],) + image_shape)\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_test = x_test.reshape((x_test.shape[0],) + image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c6dc77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will give us 10 labelled handwritten images from 0-9 \n",
    "def get_10_image(x_train,y_train):\n",
    "    selected_x,selected_y = [],[]\n",
    "    for i in range(10):\n",
    "        number_index = np.where(y_train == i)[0]\n",
    "        random_index = np.random.choice(len(number_index),1,replace=False)\n",
    "        select_index = number_index[random_index]\n",
    "        selected_x.append(x_train[select_index[0]])\n",
    "        selected_y.append(y_train[select_index][0])\n",
    "    return np.array(selected_x,dtype=\"float32\").reshape((len(selected_x),)+image_shape),np.array(selected_y,dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1088892",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_x,selected_y =  get_10_image(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bf787da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 28, 28, 1)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "print(selected_x.shape) # 10 images of 28 x 28\n",
    "print(selected_y.shape) # 10 labels for each corresponding image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d37cc9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(selected_x,selected_y,title=None,save=None):\n",
    "    ncols = selected_x.shape[0]\n",
    "    fig,ax  = plt.subplots(nrows=1, ncols=ncols,figsize=(20,3))\n",
    "    for x,y,ax_i in zip(selected_x,selected_y,ax):\n",
    "        ax_i.imshow(x.reshape((28,28)))\n",
    "        ax_i.axis(\"off\")\n",
    "        ax_i.set_title(int(y))\n",
    "    if title:\n",
    "        fig.suptitle(title)\n",
    "    if save:\n",
    "        fig.savefig(str(save)+\".png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce09237d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABGoAAACkCAYAAADPGsjRAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAryElEQVR4nO3dd3xVRfr48edJIXRCkV4ChIB0FHVBERWVRQXLYltU7F1xXXSVtaBflK8/BUWxrooFvioo1lVcK4uoiIqKdKQISO8QCCnz+yNx5kzMDTf1niSf9+vFi2fuzL13yOGce+5knhk1xggAAAAAAABiLy7WHQAAAAAAAEAuBmoAAAAAAABCgoEaAAAAAACAkGCgBgAAAAAAICQYqAEAAAAAAAgJBmoAAAAAAABCgoEaAACqGFVdoKrHlUZbVf1AVYdH+VqrVPXECHVPqeqd0bwOAABAZabGmFj3AQAAVACqOlpEUo0xFxTz+atE5HJjzMel2S8AAIDKhBk1AAAAAAAAIcFADQAAVUwwBUlVR6vqVFV9SVV356U69c7fVlX/LCKjRORcVd2jqj/m1X+uqpfnxe1V9VNV3aqqW1R1iqomR9mnF1R1TF58nKquVdVbVXWTqq5X1TNU9RRVXaqq21R1VOC5R6rqV6q6I6/tRFWtFqg/WVWXqOpOVX1CVWf+3ue8+ktVdZGqblfVD1W1TYl+wAAAACXAQA0AABgiIq+KSLKIvCMiE/M3MMbMEJH7ReQ1Y0xtY0yPAl5HRWSsiDQXkUNFpJWIjC5mn5qKSHURaSEid4nIv0TkAhE5XET6ichdqtour222iPxNRBqJSB8RGSAi14qIqGojEXldRG4XkYYiskRE+toOq54huQNQZ4nIISIyS0ReKWafAQAASoyBGgAA8IUx5n1jTLaIvCwiBQ3CHJQxZrkx5iNjTIYxZrOIjBeR/sXsU6aI3GeMyZTcQaRGIjLBGLPbGLNARBaISPe89/3OGPO1MSbLGLNKRJ4OvO8pIrLAGDPdGJMlIo+KyIbA+1wlImONMYvy6u8XkZ7MqgEAALHCQA0AAAgOXKSLSHVVTSjqi6hqY1V9VVXXqeouEZksuQMsxbE1b+BIRGRf3t8bA/X7RKR23vumqep7qroh733vD7xvcxFZ8/uTTO4uCmsDr9NGRCbkpU3tEJFtkjszqEUx+w0AAFAiDNQAAIBoHWyryLF5bbobY+pKbqqSlnmvRJ4UkcUi0iHvfUcF3ne9iLT8vaGqarAsuYM4VxljkgN/ahhjviyHfgMAAPwBAzUAACBaG0UkRVUj3T/UEZE9IrJDVVuIyC3l1K86IrJLRPaoaicRuSZQ928R6Za3GHGCiFwnuevf/O4pEbldVbuIiKhqPVU9u5z6DQAA8AcM1AAAgGhNy/t7q6p+X0D9PSJymIjslNwBkunl1K+RIvJXEdktuYsOv/Z7hTFmi4icLSL/T0S2ikhnEflWRDLy6t8UkQdE5NW8tKmfRWRQOfUbAADgDzQ3VRsAAKDyy5sNtFZEhhljPot1fwAAAPJjRg0AAKjUVHWgqiarapK49Wu+jnG3AAAACsRADQAAqOz6iMgvIrJFRAaLyBnGmH2FPwUAACA2SH0CAAAAAAAICWbUAAAAAAAAhAQDNQAAAAAAACHBQA0AAAAAAEBIMFADAAAAAAAQEgzUAAAAAAAAhAQDNQAAAAAAACHBQA0AAAAAAEBIMFADAAAAAAAQEgzUAAAAAAAAhAQDNQAAAAAAACHBQA0AAAAAAEBIMFADAAAAAAAQEgzUAAAAAAAAhAQDNQAAAAAAACHBQA0AAAAAAEBIMFADAAAAAAAQEgzUAAAAAAAAhAQDNQAAAAAAACHBQA0AAAAAAEBIMFADAAAAAAAQEgzUAAAAAAAAhAQDNQAAAAAAACHBQA0AAAAAAEBIMFADAAAAAAAQEgzUAAAAAAAAhAQDNQAAAAAAACHBQA0AAAAAAEBIVLqBGlVtoKpvqupeVV2tqn+NdZ9QNKp6vap+q6oZqvpCrPuD4lHVJFV9Lu883K2q81R1UKz7haJR1cmqul5Vd6nqUlW9PNZ9QvGoagdV3a+qk2PdFxSdqn6ed/z25P1ZEus+oehU9TxVXZR3n/qLqvaLdZ8QvcD59/ufbFV9LNb9QtGoaoqqvq+q21V1g6pOVNWEWPcLRaOqh6rqp6q6U1WXq+qZse5Taap0AzUi8riIHBCRJiIyTESeVNUuse0Siug3ERkjIs/HuiMokQQRWSMi/UWknojcKSJTVTUllp1CkY0VkRRjTF0RGSIiY1T18Bj3CcXzuIjMjXUnUCLXG2Nq5/3pGOvOoGhU9SQReUBELhGROiJyrIisiGmnUCSB86+25H7X2Cci02LcLRTdEyKySUSaiUhPyb1XvTaWHULR5A2svS0i74lIAxG5UkQmq2paTDtWiirVQI2q1hKRv4jIncaYPcaYL0TkHRG5MLY9Q1EYY6YbY94Ska2x7guKzxiz1xgz2hizyhiTY4x5T0RWighf8isQY8wCY0zG78W8P+1j2CUUg6qeJyI7ROSTGHcFqMruEZF7jTFf530urjPGrIt1p1BsQyX3y/6sWHcERdZWRKYaY/YbYzaIyAwR4Rf7FUsnEWkuIg8bY7KNMZ+KyGypRN/7K9VAjYikiUi2MWZp4LEfhRMPiDlVbSK55+iCWPcFRaOqT6hquogsFpH1IvJ+jLuEIlDVuiJyr4j8PdZ9QYmNVdUtqjpbVY+LdWcQPVWNF5HeInJI3hT9tXnpFjVi3TcU23AReckYY2LdERTZBBE5T1VrqmoLERkkuYM1qDg0wmNdy7sjZaWyDdTUFpGd+R7bKbnTSwHEiKomisgUEXnRGLM41v1B0RhjrpXc62g/EZkuIhmFPwMh8z8i8pwxZk2sO4IS+YeItBORFiLyjIi8q6rMbqs4mohIouTOwugnuekWvUTkjhj2CcWkqq0lN13mxVj3BcUyU3J/kb9LRNaKyLci8lYsO4QiWyy5M9puUdVEVT1Zcs/JmrHtVumpbAM1e0Skbr7H6orI7hj0BYCIqGqciLwsuWtHXR/j7qCY8qaVfiEiLUXkmlj3B9FR1Z4icqKIPBzjrqCEjDFzjDG7jTEZxpgXJXeK9ymx7heiti/v78eMMeuNMVtEZLxwDCuqi0TkC2PMylh3BEWTd1/6oeT+4qmWiDQSkfqSu34UKghjTKaInCEip4rIBsmdNTxVcgfeKoXKNlCzVEQSVLVD4LEeQqoFEBOqqiLynOT+JvEveRdVVGwJwho1FclxIpIiIr+q6gYRGSkif1HV72PZKZQKIwVP/UYIGWO2S+4XCNJkKoeLhNk0FVUDEWklIhPzBr63isgkYdC0wjHG/GSM6W+MaWiMGSi5s06/iXW/SkulGqgxxuyV3NHRe1W1lqoeLSKnS+5v81FBqGqCqlYXkXgRiVfV6myZV2E9KSKHishgY8y+gzVGuKhq47ytZGuraryqDhSR80Xk01j3DVF7RnIH1nrm/XlKRP4tIgNj1yUUlaomq+rA3z8PVXWY5O4Y9GGs+4YimSQiN+RdW+uLyE2Su2MJKhBV7Su5KYjs9lQB5c1mWyki1+RdT5Mld72hH2PaMRSZqnbP+1ysqaojJXcXrxdi3K1SU6kGavJcKyI1JDdn7RURucYYw4yaiuUOyZ0ifJuIXJAXk8NdwahqGxG5SnK/HG5Q1T15f4bFtmcoAiO5aU5rRWS7iDwkIjcZY96Oaa8QNWNMujFmw+9/JDdFeL8xZnOs+4YiSRSRMSKyWUS2iMgNInKGMWZJTHuFovofEZkruTPAF4nIPBG5L6Y9QnEMF5HpxhiWVqi4zhKRP0vuNXW5iGSJyN9i2iMUx4WSu8nFJhEZICInBXYqrfCUhcoBAAAAAADCoTLOqAEAAAAAAKiQGKgBAAAAAAAICQZqAAAAAAAAQoKBGgAAAAAAgJAodMvjk+LOZqXhGPkoZ5qW1mtxHGOntI4jxzB2OBcrB87Fio9zsXLgXKz4OBcrB87Fio9zsXKIdByZUQMAAAAAABASDNQAAAAAAACEBAM1AAAAAAAAIcFADQAAAAAAQEgwUAMAAAAAABASDNQAAAAAAACERKHbcwMAAAAoO5vf6Wjj+Dh/h9wGpy0t7+4AAEKAGTUAAAAAAAAhwUANAAAAAABASJD6BKBQ8Y0a2rjmm2rj19r9J7rnqz8enPrK1Tbu+NAqry5r/YZi9BAloQn+x0D8IY1svOKKdjZuc9xqr92/O77rnpPvGJ+1/CQbL5iZauO2Y7732pmMjGL0GEC04uvWtfGGYV1svL1HtteuQYsdNr429b9e3fQTD7Nx1rrfSrmHVVNcz85eec7hL9u477zzy7s7AIAQYkYNAAAAAABASDBQAwAAAAAAEBIM1AAAAAAAAIQEa9SISELTJjbeeXSKjded5G+RuHLIMzaevT/Hqxt9yWU2jps5r5R7iGjEN2ls4+WPNLPxm32e8trdctTpNs7euKnsO1bB5BzT0yvXGbvWxq+0+9C1i/YFjd9y0XmP23jEsUd7dasHu3Mxa8PGaN8BRbT56j42rjZks1c3q8drUb1G8KjmGH+9i9faz3CF9i48/ZjBXrsD9zS1cfzn/vo1AJzMEw+38drLsqJ+3rDOc218RyN37Z2d4f+e7on1J9j4kUlneXWtMpZE/X6ITq9JP8e6CwAKkH2cW5Pr4/973q8z0d35jt/ewcafD+nm1WWtWFX8zqHKYUYNAAAAAABASDBQAwAAAAAAEBJVJvVJk5JsvOKew7y6iUOftXH/GukRXyPTuHGt3kn+9Lca9663cUb/YncTJbDjeLeV8KJjn7DxqqzsgpojgvRmSV45c19tGw/4eWjE5+19I5DGEth1eXMf/+f/8aDxNp7QfLZX1/F/r7Rxh4tJfSpNwWvgdSPetPFFddeV+LVHbeztlQcnu/TPPknu+L+d9q7X7pNna9r4sZP+7NVlrfS3A0fZ+vWuvl75p6ses/E3GWrj0cMv89rFzSLVt6zEd3Rb29/+9CQb96vupz49tK2jjZ+e18+re/cxd0Pyrri48Sw/5TF7yXIbN5cv/bqidBoRBdOzu9ecG7FdxmeN8j2ytIx6BGD7xX28cv8bv7Zxpine1e+m+u6cnXT+QK+u1X2rivWaqJqYUQMAAAAAABASDNQAAAAAAACERKVNfYrv0tErN3rWpSa93frR8u4OykD8oR288raz9hbY7rWdh/sP5JgC2yFX7Wlz/AemRfe8GrKywMfrv+iXT7vrVhsH0ytERKb1f9LGd3a50MbZC9h1pDTVid8XsW5RZqaNz/j8Ohu3eCfyx0XNt771yvN7DbPxuhPq2fj7m/zjPSCQajqhbk1B4RJatbRx+vPxNl69tKnXrsN1+c7hKGi+y2JOYF+v3oFsyHX9a3jtWs0q8lshSouvdSkwwXSnXnOHee1aXvybjTvsiG73NNKZykdcnTo2rvm6O6fOrO3vODlldwsbt3xugVfHsQJKLq5rJxsvuzjZxl+e+5DXrn5c9VJ9396n+Tu8bbyvVF8elRwzagAAAAAAAEKCgRoAAAAAAICQYKAGAAAAAAAgJCrVGjUrx7ot1iae/axXd3yN/Tb2N9YuHQ+lvGHjYZeMtHGDSV+VwbtBRGTTMf4WlguOftzGOeIWXJg8dYDXrtVmf+tRlK92zwe2Xb7Kr+teza27sfiaZBt3uL6MO1UFmAy3Z/r9E90aF//oleG16zTBrV/T4Yfvivde37k1FvZc17uQliiKbf3cGjWfdw6s99PZbzfkuiNK9X0/3ufW2UiZvsWrY/2MspOwTwt8vHXyDq+ck1itHHqD4ohrkGzjV9q9Fazx2v2/l4fauNUO7lGiMWXNbBvXj3NrZ80/kOm1u+znCyUaB2a5e8rqm/1Fu/a0cudi7TWuLr2pf45W7+tfHyM5J2WejUc28Nfg25Xjvq8Muu1mr67elK8FxbP4Rvc5tvTUxwM10a9JE1y77Zb1faN6zuzl7b1yqsyL0BK/23X+n2y899ydXt3Yrm/a+NSa7lzJNv63+6d2trHxJ1s6eXUrX3VrnDZ+ItzXW2bUAAAAAAAAhAQDNQAAAAAAACFR4VOfVo1x6U5zLhhn45pxiflaujGpORl+3fD/XGnjukvcj6Tl9DVeu5w6tWx80qvfeHXX1XdTF9+850EbX/XxeV67rDVr//BvQPQSmrltaNsNXxqx3W0b3NT/VmPCPa2tqslau87GJy88y6v7uLOb0pjQKPIW0iiZJo+6c6JJvrrSSA3dcZG7Ls888cFAjb+185TdzWwct3VXqfcD0et9ys8R69YcaGjj7IWRr7soXe3udlttpyZfbePlg5/y2p07/WQbb37gSK+u+nv+vQrK18Lb3DUumDYxeVcrr13K1A02Jp0wOjevHWTjSa0/t3G3av49/jeHvRrdCx5WGr0qubqB7aHPGzXDq/tgSnI596by+H7QhEApKWK7nYHUs6Pe+LtX1+BHl+oW7dIWVS3Vaf9p7jNoT/N4r67RM+5nFp/a1qtLf8KlFM7o/LCNf8vy0xAvWXiRjUd92DhiP6pvd8+77Pa3vbq/jnrXxj3b32Tj9n8PX2ohM2oAAAAAAABCgoEaAAAAAACAkGCgBgAAAAAAICQqxBo1muC6ueJef9vRBcMnBkqRcw4f2Z5m44+71vHq0qTgHO6sQvr04rN/9soX/X2+jZvEu3UYcurV9p/oL3tTpegR3Wxs5s4vpGXwSf7WhxtPczmNc9o+nr+1NWuD2w6vviyLsocoF3EuZ7VGgr+NZnBbdVQc8fXre+VjRsyxcbP4GvmbWy/cfLqNk9bOLf2OVTIbjy678+O51p955eAaQeN+OMnG7eSHMusDfCYjw8Ydrw+sVyNXe+2Ca9aMuLuPV/fLx269i5z9+wVla/tw/+e/YHBwXQx3P/PKtad47eKXfS8omi2D3M8zbeJlNj42dXnUr3FHsw9snJJQs3Q6FsG67PQCH28RH/l9z63jrx32gRxTqn2CyOM7/O2zJz/s1j5KfTa6dWjg1J6/3sZJ2xt6dQlN3aqI933krx2117i1pfo97NYGajnZP5/rbXTlehLduf7GS/5aNo+9eZyNU3u5L+Zh/AbCjBoAAAAAAICQYKAGAAAAAAAgJCpE6lMw3Wn+8Ee9ukhbuPaYfalXbnNOlKk2UWr6iL/l8+Vnum2GX0t9z8aLr6/rtUvzZyxXesVKdwqIS/LT2eaMjpzuFLR1mZtuR+pTuCQ0d1usv5X2TsR2OevKdhoySiahjdte9v9mT/Xqamq1Ap9z/5ZuXrnWwo02LizVFLn+ecLbB29UBL889KdA6buI7Q55p3rEOpQPk+XOkLSr/XTtq3v1s/HElp97dZd96tLWNvcl9aksxCfXc/F5m/y6QPr2o9s72bja/NVeu9LYkjv9zKMi1tVZusO914IlpfBusZe9Y6eNUy9wWyD/VoTXuLHVeTY2SYmFtCwFgW3Dk/+12cYvp3wS8Sn/s/HEfI9wDhfXWYvPt/F/Ok+38edbOnrtGpLuVCJZq10qka721/v45Z6+Nu5WzT/fjr/OfUFu9pb7jl3YtTG+rvuOnd05xaszce7auyPN/z6xL5Axta2zW4LBT+IPB2bUAAAAAAAAhAQDNQAAAAAAACER2tSnVWPcyvnBnZ0ipTqJiHSdeYWNUy9d7NUV9jyUnWKlO9WqZeNlo7vnq/1SIgmuqN/xmW02Lo0pxSg9S/7WKmLdzhw3rTfl3QPl0R0UIq6rm6q/eIS/W96XAx+2cU2NvLPTPZt72vj7oaleXfaqlSXsIYoirrqfwpTUdneMeoLS9OtRe2385IIOXt1TrWfY+Nir/2bjQ55ien9pWXzPoTZe1P0xr25zttu56+27XRpLrS1zpKSWPeanOi056wkb5+S7631vr0sHf/bsU127HxeVuB8VWdaateX2XqZvDxsXlu6UYVya46yph3l1zQu5B0bhNnzRwhU6uzC4XIWIyICh19u41uslP0/hNPzZfSPLyvftbOIjbmmTd+7taeNZm/37xnOaf2vjLkkuZbtmnL+LbFCXRD8df2mm+65x3ZU3HqTXscWMGgAAAAAAgJBgoAYAAAAAACAkGKgBAAAAAAAIidCsURPXvZNXnn7B+ECp4K1e82sb2Lk5Z384trCruyQ0P+IK48BR7v/Ckr9Gtx23iEj/T0bYOG1h5K1mUb4SUlp75afP+FfEtq/sconD8Z9/X1ZdQkBwe0MRkUXj02z85gB3/nWplv9aFnldmk6fXm7jjve6LVSzl68oZi9RmGPmDfPKDWRpge3Moe298vd/eiFQ4vc2lcGH5//JK1/5vvu/MOOfD9l46Pq/ee1qvO1v+Y3CbbvEraO4bKi7Ti7N9NdJGPrMrTZu+UbJ1xc5MLC3jaef9qhXtzPHrW3y5f5DvLohtbbbeOyRyTZu+GOJu4Qobele8+CNRGRpprFx8wdZk6a0JC916zZtzN5n4ybx/r3MKXd9buNpTQYU673Ou/ojG/eo8atXN+a2i21c1dbAqT3N/XtP3n+dV7fuOHcP0rfvQhsvXdXUa/fIjLNs3Ph7twZY4kz/YqYJ7p71ocWfeXUf7XXfNRL/862EGXdmAAAAAAAAIcFADQAAAAAAQEjENC8nvktHG58z9VOvLjUx3ClDG27q65U/avdgoORStVpOX+O1yxIUJGPQETZ+71/B7S0jp731mHOhV+54hZv2ZvI3RswsfyDZKx9X3U0N/+GAvz3fmyPc9qWJQvpaecjunOKVlw56OlAq5nVYi90dBOT06+WVuyQ9a+O4wO9Z4uP8rXh/vct9PmngYpjR0G8XfI1EjffqArPvxXA8K4ycnxZ75cO/uszGP/d90ca7W/jnduRERoiIJLRp5ZXvHOV+ljmBO45T3/VTyjrcX/LUlfgmjW085slnbHxoNf93rUeMu8XGiXv8u6DWtz9i4yYz3H0p96RlJ656da88+Jr/RvW8v7zl0vhT5etS7VNVVv+HrTb+LTvJxk38jz65paFLu7ll1EIpbaOr84EqIlL9XT/dtv27Lt4YeDxNoktN+sP3vnh3YPcb/yB3SNpg4w8kOarXjxVm1AAAAAAAAIQEAzUAAAAAAAAhUa75RZqU5JUX3VDPxsPqrM/X2o0hdZ10vY3/cupsr9U9jefZOLNOoo2j2yeqaOIbNrDxJVe879XVj3NTHLcEVhOXHJJwoqE3b7JxDY3u6LUc648zmiwm8YZF5omH2/itox7z6nLEXQfOeftGry71Y6b5lre4n5Z75bT/XGnjpSc/k795VBYf71J0xnTrbuO5Qzt67bKXsQvUH6ibFj36xee8ql5JLnUpmMQ0s8crXru4HnGBdn66U1CwJtPkr3O12zu51/P3CEPYtb3b7YohbiMS6TjMT5Ha/kQ5dagCiU9296grHkr26gbWdLvZ3b+lh43TRv7gtSvOHWBcrVpeedeLtW3cO8mlCw+88lqvXcuVW2w8aJq/m8yVCy+wcf01y4rRKxRVxjFdvPJdjQre8fK7fCngHe//xcbZ+RvjD+K6up1i97esY+OG/1zptTuxoUtD7FWt5PMUZu933zl35fhpbqMXDbZxjZeTvbqGX5F6WC4C378XH2jmVSXH7y3v3hQbM2oAAAAAAABCgoEaAAAAAACAkGCgBgAAAAAAICTKdY2aFfcc5pUXD37Uxvmz6HvMvtTGKXd8ZeMZ64722t19h9vCN26k29BLP/PXwzEZGVJSy//u1ld4M/lDr25l1gEbX3C32yKx/pqvBH8Un9rWK49t/3qgFHnruivXHGvjuFUbvDpyeWMruIbTaRM+tnFaop+7+9A2dx51muCvTUW+bun57da+XrnFBHetDF4Pc9LTvXadrllg4zOanmnjpde28Npl1XdHa8Ggx7264FbPdzT6ycYPvOGfpV+d7v4vZK1cXcC/omoLrkcRS/0HubXgVt0dw46g6NZvLvDhS5vM8sqPdDnLxtkLlpRplyqK3y50a4z80HdCxHbvPdLfxg0yinfPF1wPZ/srjby6d7u8ZONuL420cdvNe7x250z/3Mbf7G7n1TU8e52NI69ahdK09sTo1lv816bjvHL25oLP2aosro5be2Zfv05e3c0Tpth4UM3dZdqPdOO+6903/DIbJ+w54LVrutrd22Zv96+n3OeWv0StuD91ZtQAAAAAAACEBAM1AAAAAAAAIVHmqU9xNWvauPvR0W8J2P6WHTYOTlhqOtWfQta79ggb70nLtHEn3eS1K84WielnHuWVFwyfaOP8U0dP+cJtId7+BdKdCqK9u9p431h/euIRSQWnO2XlS2ha9U+XKpGw+bv8zRFDcW+4dMOrk4PbLvvH9tl/n2jjdis5V6KR0KK5jRfd2trGowe+7rU7vbbb9jHHfOnVDXtmkI2zC0kFzdm/38WrfrVxu1t/Lai5iIgcN3yEV559/8QC2/2j4QKvPLC9S2VNJPWp2I6ZN8wr7/26UYHtMhr6n1wLz3ksqtefP95tP1xHvi5i76oGTfBvp4JT9c0Bf1p8zt7Ybw06oIZ/DRjdrb6N6yzI37pqiOvZ2SvfecPkiG27vnyjjdtNKvnnWPpU9/P/vMurXl3vh106faO17hwe+Pxsr13PJHf9f+nGIV5dYvq3Je4jDs70cdfKL4Y9lK+2RoHP+eLfPbxya/mywHZVmenYxsZXPDzdqytOutPOnMB9Tr66j9LdPdaCdD/le9Qh39h4+aXumt/pBv/7bXYIrvFVnnFHdne2f+7Viiv5cijlhRk1AAAAAAAAIcFADQAAAAAAQEgwUAMAAAAAABASZb5GzS93uNzL+e0ejdjuzwuHeuWk39YX2C576zav3PzBgnM5i7v94MqxfWw854Jx+WrdGhzXrjneq2n3OBseHsyqIS5nf0Hnlwpp6XSfdKNXTvmENU3C4reR/vbP/20fzMd258oxP53ttUu972cbc9ZEZ9VFKTZeMjTyuiLrs92aTueMGunV1dtRumuLxDdqaON+N84p1deu0oxbUW1IiyP8uj91t+EvQ936b+1H+se2gSwt8KW1VxevHHeO+11NcEt1EZGpexrYuM4K8u0PZveZh3vl/z7ypI17jbveq2s2PjZrUMSrO94z0pO8uuQft9o4HJvCl79NR9bzykNqbbfx+mx/TYPUV3bYuDifY9veS/PKc7tOde+7bLBXl5Po4pnjHrfxgJ/9++YZ9x5r48TZrEkTC6tOd9flhnEFr0kjIjJld2Mbt76XNWkO5tQXZ9n4nNqbCmkZnYtOuNDG2ctWRGy36W1/K/B7Gs+z8dKBT9v49Oon+U9kjZqY02rVbHxx3d+8uhn7auZvHlrMqAEAAAAAAAgJBmoAAAAAAABCosxTnz6+4MFAyZ9q++YeN/Wv+kh/GlJOpr+dZVlaNabgdKeacYleu/XZ+2y8+nZ/2mr8l9+XUe8qth0XuZ/tF5cEU2P8KaHBKdlTdruUitTn/RS4LEF5C6a4rLzebY/+8xX5t2CubqMHth5q43p3VPda5ewu+laKVd2+5tElIwz+/gobN51c8lSn+EMOsfFv53Xw6k6/fKaN72j0U1Sv92vWPq+csJczuki+dj/n9sU4vGvv9Ms5gaSNTOPXPbemnyt8M7/ob1YFxNV09y2tbvK3Zw1eA1v93y9eXXn+r19xk+tHtvnExte9d4nXLnUR265nnbLDKwfPjwFf+Olr7X78ocivv/UKdz80rduDXl2mcfdE01Lf9eq2XePSrvrOu9TGh9zsfy5kLy16n1Aypq+/tfbMvwaPa+T0ion/61LC6wsp/WXhtg1+6vCiUxrZOHvz6lJ9r50D/O+EtadyPY21nO6pgdIXMetHSTGjBgAAAAAAICQYqAEAAAAAAAiJMk99ap1Q28aZxp+m+eqGI22c8+OiMu1HXHe3cvfqu/x/9s99gikcLj1r8JIhXjtzwjobxwupTtFIP3OnjesXsgJ+tnFTjCef7VZPz1mxuGw6Bo8mutXR95/kT+VtdPtKG//Uzu04lH+ni08Cq6h/cZ57jfhdO/yGLVvYMKNDE6/q15MC6ZFt023Yr62fOjDr0242Tnkv3avTL3+Uyqbjc4F0sbMitxvfze0cMmGmvwvBlidTbBwXyHNZN9A/kt06rrFxr2QX39FoRrTdjei0b67xyq2/qnzHKsxa199+8EaIXlqKDaekTPaqbtvodoEyGeWXyr36Hn83vvmXu2v29pz9Nm42K1+uG6Rb44J3GxURqfGdn8aS0KaVjc0et8OL1q7ltdt4cksbv3+XS/+uF+cvBRBMCx236USvbt74njZu8IpLqaiqu3OFycoz/P8XjeMjpztd8utxNm4wea6NORMP7qWVR9n46l6Rd2kKeueTo7xyu41ll2K2YUi+XeGmRmiIcrOlR62Idbf++Bcbt5QF5dGdYmNGDQAAAAAAQEgwUAMAAAAAABASDNQAAAAAAACERJmvURNclyYn36oWOaJFfr24OnW8clbP1ALbrbzaLy867tmI/ViZ5fLHz553uY2bD/vVa0ce6cH98mAfr7zoyOD6P5GPd8eZbsvJ9vNZt6KsHRjY2ysn/cPl5n/U8SmvLi5w3PKvSxO0cL9beyb9EXdOfdLF32o0pxhnUly+/zs5w93W0P89t5pXN+7I/jbO3rqtyO8VSsvcVpKdPnPXqMXHP+s1O7a6+7kfm/pv/zXGlU3XDubPi860cdtrN3h1rLGAiixul1sf65N9/poj/9vkOxt3vvk6r65O4Nai0QuuncmMvJZNQmBtr42DWnt1zS9w64h92368V5dp3O/jBoy/xcZN3/gy4ntVVT9tauY/0MaFc2+e4Nfd7MJXdrtjc36ddRKZ+6z6eJ9/Lzv6vhtt3GCSv5ZGXWGr3zAJbsk949wH89VGXqNm6cTONq6XxTEtilpP1LPx0if862RaYrX8zUVEZNyZL3rlTwe4n/9XE9zW3Um7/DuR3c3dV+Onu+c77yPMb0hYXb3AxxE72/tE/jyNn10vYl3YMKMGAAAAAAAgJBioAQAAAAAACIkyT30qzB2tXUrE/TNP8+rWvdDOxq0uWW7j5jV2ee3GNX8myndzY1Ljtnb1at585AQbN33eTTktLM0DTlwttwXaOSfO9usipDu9m17XK6eNcNOFsw1JZmUhoUVzG0951p/OWS+u5NM2b6i/rMC4sJS3lVn7vfKHezoX2C5O86VNBqbzf7TlUK/OHNgrlU3OXvdvSrtqqY3/PvNPXrtxzWIznXrURj+V7rMnXb8av77Yxtnb2R46luLUv7bGBT4XEzXeq9NAW67IBctascrGo/95mVc3YPwTNl546eMRX2PayIY2zi7kd2eHxLstRAfUyIjYbvz2Ll750wvcFrVNfyDdqTCNJ9Twyt2ucsd0fr/nIj5vWB2XOvzSrlZe3ZiZQ2yctNHdcrf5YI/XrsHXZbd1MErXsuEu1SYlIXKq0wNb/XuT5Nfn2ZhratEkve+2M7/umhu9uqwRW238WbdpNh5Uc7fXblDNOa4wdo5EJ7r5DO3HL/HKpHWHW4uP3bIIYf+uz4waAAAAAACAkGCgBgAAAAAAICQYqAEAAAAAAAiJMl+jpusL19v4p4sf9ep6BHZUey31Pf+JY1wYzKPPv7V2JI9u7+SVX5400Matpvzi1TXYQG5wicS541MvYV9UT7nzXxd55eabyZ0vaybdHZuZ+/xtSDtU22TjoV9fWab9aPiOy+mut8xfT8bMnV+MV9xw8CaVSHC9muUDG3h1ZySeYuOdx6R4dR1HujUunmo1U4pq/Db/mvrUrONtfOgoPz+74Q53TSVXO7YSWrW08aF1/a2Dg5+nmfkWTViy1G05nCaFbTkMEZG607/3yidvvsLGK4b66//06uq2057W/sMiv9fLu5t65dfOO9HGumKtV5eza2GRX7+qiv/MP4ZtP3PxEDlCiiNNvilJlxASW67qY+Plp0Vec2pTdrqNpz96glfXMIPvGqUh6YO5XrnmN+4+6JSpZ9j4/U5vlWk/jvx2mI2b7FlZSEvE2q4cfz1MXb81QsvwYUYNAAAAAABASDBQAwAAAAAAEBJlnvrU9m43Re3ILSO8utuvfsXGZ9beJJGsz3YpG/3fvzliuw4vui0rExb/6tU12+5Sa7IK6S+KLme32wLvg/X+1qAjG7iUiPNXnmTjFuP96cBsVVj2glsjP5PWLl+tK7eVn8qpRxz3ksreui1iXa3XN3rlta+7+DQ5vMTvHZzST3pTxbcoM9MrHzrBXS84vgdnMg945YRPv7Nx2qd+22DC5ylyWCm8O+lNQGkyfXt45WE3RJeieNXKoTZu+CypTuUheB+UMNiluHQddb3XrtmR62386yKXPvrUqc957XICcxgmb+rj1X37UWcbt7nbfa/kXjZ8Luz1tY3v33y0V5e9eXN5d6fYmFEDAAAAAAAQEgzUAAAAAAAAhESZpz6ZLJdo1PRhf2efSQ+3cbG0kWhEu4I+U7VjI+nkVV7Zn9ZdcVbZBoDKIGuN2wXowyl9vboxN7vP09M/9aeJpy36tmw7BgAhtfyv1b3yiPrLbRyv7nfcb+2t7bXLuqZuoFS1dqQMg5x0t+tWyh2RU886yCobjxvRJWI7kR1eqY2wQ21YxR9yiFc+te77Nj5/tr+bbarMK5c+lQZm1AAAAAAAAIQEAzUAAAAAAAAhwUANAAAAAABASJT5GjUAACD2mo3z8+uHjDvCxmnCmjQAqq6cfr1sPO3Ux/LVuq9L2SbHxv+YeqHXKmUhW3IDsbB1UKpXPrxavI2bvl2tvLtTaphRAwAAAAAAEBIM1AAAAAAAAIQEqU8AAAAAqqzl5yfauGe1yF+Pbt3Q28Zt757r1ZnS7xaAKOxtpl55V85+G9f7aLFXl10uPSodzKgBAAAAAAAICQZqAAAAAAAAQoKBGgAAAAAAgJBgjRoAAAAAVVbatd/Y+JRrDyukZU6EGECstHjgS6983gN9A6Wd5duZUsSMGgAAAAAAgJBgoAYAAAAAACAk1Bg2kwMAAAAAAAgDZtQAAAAAAACEBAM1AAAAAAAAIcFADQAAAAAAQEgwUAMAAAAAABASDNQAAAAAAACEBAM1AAAAAAAAIfH/ATmH1mvDzUgNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1440x216 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_image(selected_x,selected_y,title=\"initial image\",save=\"initial image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52c4245",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3b82389",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-11 18:40:47.433792: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-11 18:40:47.434168: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "# input shape = (None,28,28,1)\n",
    "encoder_input = keras.Input(shape=image_shape)\n",
    "\n",
    "# input shape = (None,28,28,1)\n",
    "# output shape = (None,28,28,32)\n",
    "conv_1 = keras.layers.Conv2D(filters=32,\n",
    "                             kernel_size=3,\n",
    "                             padding=\"same\",\n",
    "                             activation=\"relu\",\n",
    "                            )(encoder_input)\n",
    "\n",
    "# input shape = (None,28,28,32)\n",
    "# output shape = (None,28,28,64)\n",
    "conv_2 = keras.layers.Conv2D(filters=64,\n",
    "                             kernel_size=3,\n",
    "                             padding=\"same\",\n",
    "                             activation=\"relu\",\n",
    "                            )(conv_1)\n",
    "\n",
    "# input shape = (None,28,28,64)\n",
    "# output shape = (None,28,28,64)\n",
    "conv_3 = keras.layers.Conv2D(filters=64,\n",
    "                             kernel_size=3,\n",
    "                             padding=\"same\",\n",
    "                             activation=\"relu\",\n",
    "                            )(conv_2)\n",
    "\n",
    "# input shape = (None,28,28,64)\n",
    "# output shape = (None,50176)\n",
    "flatten = keras.layers.Flatten()(conv_3)\n",
    "\n",
    "# input shape = (None,50176)\n",
    "# output shape = (None,128)\n",
    "encoder_output = keras.layers.Dense(128,activation=\"relu\")(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b866973",
   "metadata": {},
   "source": [
    "## Latent Space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08bd1c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input shape = (None,128)\n",
    "# output shape = (None,latent_dim)\n",
    "z_mu = keras.layers.Dense(latent_dim)(encoder_output)\n",
    "\n",
    "# input shape = (None,128)\n",
    "# output shape = (None,latent_dim)\n",
    "z_log_sigma = keras.layers.Dense(latent_dim)(encoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "185967ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    z_mean, z_log_sigma = args\n",
    "    epsilon = k.random_normal(shape=(k.shape(z_mean)[0], latent_dim),\n",
    "                              mean=0., stddev=0.1)\n",
    "    return z_mean + k.exp(z_log_sigma) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35790214",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = keras.layers.Lambda(sampling,output_shape=(latent_dim,))([z_mu, z_log_sigma])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c2b91a",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a47dbdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input shape = (None,latent_dim)\n",
    "# output shape = (None,128)\n",
    "dense_2 = keras.layers.Dense(128,activation=\"relu\")\n",
    "\n",
    "# input shape = (None,128)\n",
    "# output shape = (None,50176)\n",
    "dense_3 = keras.layers.Dense(np.prod(k.int_shape(conv_3)[1:]),activation=\"relu\")\n",
    "\n",
    "# Reshape layer\n",
    "# input shape = (None,128)\n",
    "# output shape = (None,28,28,64)\n",
    "reshape = keras.layers.Reshape(k.int_shape(conv_3)[1:])\n",
    "\n",
    "# Deconvolutional layer 1\n",
    "# input shape = (None,28,28,64)\n",
    "# output shape = (None,28,28,64)\n",
    "conv_4 = keras.layers.Conv2DTranspose(filters=64,\n",
    "                                      kernel_size=3,\n",
    "                                      padding=\"same\",\n",
    "                                      activation=\"relu\"\n",
    "                                     )\n",
    "\n",
    "# Deconvolutional layer 2\n",
    "# input shape = (None,28,28,64)\n",
    "# output shape = (None,28,28,64)\n",
    "conv_5 = keras.layers.Conv2DTranspose(filters=64,\n",
    "                                      kernel_size=3,\n",
    "                                      padding=\"same\",\n",
    "                                      activation=\"relu\"\n",
    "                                     )\n",
    "\n",
    "# Deconvolutional layer 3\n",
    "# input shape = (None,28,28,64)\n",
    "# output shape = (None,28,28,32)\n",
    "conv_6 = keras.layers.Conv2DTranspose(filters=32,\n",
    "                                      kernel_size=3,\n",
    "                                      padding=\"same\",\n",
    "                                      activation=\"relu\"\n",
    "                                     )\n",
    "\n",
    "# convolutional layer 4\n",
    "# input shape = (None,28,28,32)\n",
    "# output shape = (None,28,28,1)\n",
    "decoder_output = keras.layers.Conv2D(filters=1,\n",
    "                                     kernel_size=3,\n",
    "                                     padding=\"same\",\n",
    "                                     activation=\"sigmoid\"\n",
    "                                    )\n",
    "_dense_2 = dense_2(z)\n",
    "_dense_3 = dense_3(_dense_2)\n",
    "_reshape = reshape(_dense_3)\n",
    "_conv_4 = conv_4(_reshape)\n",
    "_conv_5 = conv_5(_conv_4)\n",
    "_conv_6 = conv_6(_conv_5)\n",
    "_decoder_output = decoder_output(_conv_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160aee71",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ad86b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(x, z_decoded):\n",
    "        x = k.flatten(x)\n",
    "        z_decoded = k.flatten(z_decoded)\n",
    "        # Reconstruction loss\n",
    "        Reconstruction_loss = 786*keras.metrics.binary_crossentropy(x, z_decoded)\n",
    "        # KL divergence\n",
    "        kl_loss = -0.5 * k.mean(1 + z_log_sigma - k.square(z_mu) - k.exp(z_log_sigma), axis=-1)\n",
    "        return Reconstruction_loss + kl_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f625da",
   "metadata": {},
   "source": [
    "## VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2963012f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 28, 28, 32)   320         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 28, 28, 64)   18496       conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 28, 28, 64)   36928       conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 50176)        0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          6422656     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 10)           1290        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 10)           1290        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 10)           0           dense_2[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 128)          1408        lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 50176)        6472704     dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 28, 28, 64)   0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 28, 28, 64)   36928       reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 28, 28, 64)   36928       conv2d_transpose_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 28, 28, 32)   18464       conv2d_transpose_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 28, 28, 1)    289         conv2d_transpose_3[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 13,047,701\n",
      "Trainable params: 13,047,701\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "variational_encoder = keras.Model(encoder_input,_decoder_output)\n",
    "variational_encoder.compile(optimizer='rmsprop',loss=vae_loss)\n",
    "variational_encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57eb6a6",
   "metadata": {},
   "source": [
    "## Plotting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "619d648f",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "`pydot` failed to call GraphViz.Please install GraphViz (https://www.graphviz.org/) and ensure that its executables are in the $PATH.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/pydot.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, prog, format, encoding)\u001b[0m\n\u001b[1;32m   1925\u001b[0m                 \u001b[0marguments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1926\u001b[0;31m                 \u001b[0mworking_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtmp_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1927\u001b[0m             )\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/pydot.py\u001b[0m in \u001b[0;36mcall_graphviz\u001b[0;34m(program, arguments, working_dir, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m     )\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    799\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    801\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1550\u001b[0m                             \u001b[0merr_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1551\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dot': 'dot'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# to check the pydot/graphviz installation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/pydot.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, prog, format, encoding)\u001b[0m\n\u001b[1;32m   1932\u001b[0m                     prog=prog)\n\u001b[0;32m-> 1933\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1934\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] \"dot\" not found in path.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/db/96q4k_9n5s9fk3my0k61nlhr0000gn/T/ipykernel_45945/3696155950.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariational_encoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mto_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"variational_encoder_L{}_E_{}.png\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshow_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[0;34m(model, to_file, show_shapes, show_layer_names, rankdir, expand_nested, dpi)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \"\"\"\n\u001b[1;32m    239\u001b[0m     dot = model_to_dot(model, show_shapes, show_layer_names, rankdir,\n\u001b[0;32m--> 240\u001b[0;31m                        expand_nested, dpi)\n\u001b[0m\u001b[1;32m    241\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextension\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mextension\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[0;34m(model, show_shapes, show_layer_names, rankdir, expand_nested, dpi, subgraph)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0m_check_pydot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msubgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dashed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         raise OSError(\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0;34m'`pydot` failed to call GraphViz.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0;34m'Please install GraphViz (https://www.graphviz.org/) '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             'and ensure that its executables are in the $PATH.')\n",
      "\u001b[0;31mOSError\u001b[0m: `pydot` failed to call GraphViz.Please install GraphViz (https://www.graphviz.org/) and ensure that its executables are in the $PATH."
     ]
    }
   ],
   "source": [
    "keras.utils.plot_model(variational_encoder,to_file=\"variational_encoder_L{}_E_{}.png\".format(latent_dim,epoch),show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100d3f79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcce9eee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
